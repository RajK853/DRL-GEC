# Algorithm parameters
gamma: 0.95
# Training parameters
lr: 1.0E-5
batch_size: 32
update_interval: 200
dropout: 0.1
episodes: 200_000
# Evaluation parameters
eval_max_iter: 10
evaluate_interval: 10_000
record_output_interval: 100
dev_data_path: "data/processed/wi+locness/dev.json"
model_path: &model_path "sl_results/pretrain_sl_02_12_2022_15:01/model-best.pt"
log_dir: "rl_results"
env_kwargs: &env_kwargs
    id: "gec_lev_dist-v1"
    datasets: ["wi+locness"]                   # Datasets to load
    correct_examples_percent: [1.0]            # Percentage of correct sentences to load
    reward_config:
        scale: 1.0
        correct: 10.0                          # Correct = same final and reference tokens and all labels are keep
        fn_penalty: 0.0                       # False Negative: final and reference tokens not same and labels are only keep and unknown
        out_of_range_reward: -10.0             # Penalty when the number of tokens is too low or too high
meta_data:
    base_model: *model_path
    env_config: *env_kwargs
    description: "
    Finetune the CE (2M) pretrained model. 
    Dataset has no unsolvable examples.
    Dropout enabled.
    positive reward for success; episodes ends when all keep predicted for correct sentence
    Don't shuffle batches.
    Explore-Exploit determined per episode.
    Label = UNKNOWN if len(candidate_labels) == 0
    Label = candidate_labels[0] if len(candidate_labels) == 1
    Check all append for insert edit + [keep]
    Check all labels for candidate labels for replace edits + [keep]
    Exploit = sample from candidate_probs
    Explore = Sample from candidate_lev_probs
    No return rescaling
    Early termination for false negative too.
    eval_max_iter set from 5 to 10.
    "
