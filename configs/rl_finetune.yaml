# Algorithm parameters
gamma: 0.95
# Model parameters
dropout: 0.1                                  # Dropout rate before the final classification layer
# Training parameters
lr: 1.0E-5
batch_size: 64                                # Batch size
update_interval: 200                          # Episode interval to collect the samples and update the model
episodes: 1_000_000                           # Total number of training episodes
# Evaluation parameters
eval_max_iter: 10
evaluate_interval: 10_000                     # Episode interval to evaluate on validation dataset
record_output_interval: 100                   # Episode interval to save generated text outputs
model_path: &model_path "sl_results/pretrain_sl_02_12_2022_15:01/model-best.pt"
log_dir: "rl_results"
# Environment parameters
env_kwargs: &env_kwargs
    id: "gec_lev_dist-v1"
    datasets: ["wi+locness"]                  # Datasets to load
    only_solvable: true                       # Load only solvable examples
    reward_config:
        scale: 1.0
        correct: 10.0                         # Correct = same final and reference tokens and all labels are keep
        fn_penalty: 0.0                       # False Negative: final and reference tokens not same and labels are only keep and unknown
        out_of_range_reward: -10.0            # Penalty when the number of tokens is too low or too high
# Meta data parameters
meta_data:
    base_model: *model_path
    env_config: *env_kwargs
    description: "
    Finetune the CE (2M) pretrained model. 
    Dataset has no unsolvable examples.
    Dropout enabled.
    positive reward for success; episodes ends when all keep predicted for correct sentence
    Don't shuffle batches.
    Explore-Exploit determined per episode.
    Label = UNKNOWN if len(candidate_labels) == 0
    Label = candidate_labels[0] if len(candidate_labels) == 1
    Check all append for insert edit + [keep]
    Check all labels for candidate labels for replace edits + [keep]
    Exploit = sample from candidate_probs
    Explore = Sample from candidate_lev_probs
    No return rescaling
    Early termination for false negative too.
    eval_max_iter set from 5 to 10.
    "
