{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd200d49-a213-42e7-bc8c-d184e24ca70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm \n",
    "from typing import Union, Dict\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5bd9a1-a00e-4e49-9015-bcaa77f57768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rajk/Machine_Learning/DRL-GEC\n",
      "/home/rajk/Machine_Learning/DRL-GEC/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import m2_to_json\n",
    "from m2_to_json import Speller\n",
    "from src.utils import write_json\n",
    "%cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76afb1f2-6f34-48d4-a9ad-7d020c260f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parallel(path_a, path_b):\n",
    "    with open(path_a, \"r\") as fp_a, open(path_b, \"r\") as fp_b:\n",
    "        for line_pair in zip(fp_a, fp_b):\n",
    "            yield line_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4598d8-06ae-4a91-9492-618a2fee0baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sent(\n",
    "        text: str,\n",
    "        reference: str,\n",
    "        checker: Speller,\n",
    "        min_len: int,\n",
    "        max_len: int,\n",
    "        min_sim: float,\n",
    "        only_proper_sent: bool,\n",
    "        spell_check: bool = False,\n",
    ") -> Union[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Process a given sentence\n",
    "    \"\"\"\n",
    "    # Filter sentence with ellipsis\n",
    "    if m2_to_json.check_ellipsis(text):\n",
    "        return \"Ellipsis\"\n",
    "    text = m2_to_json.clean_text(text, is_ref=False)\n",
    "    text = m2_to_json.remove_parenthetical_text(text)\n",
    "    # Filter sentence based on number of tokens\n",
    "    num_tokens = len(text.split())\n",
    "    if num_tokens < min_len:\n",
    "        return \"Less Tokens\"\n",
    "    elif num_tokens > max_len:\n",
    "        return \"More Tokens\"\n",
    "    if spell_check:\n",
    "        text = m2_to_json.correct_spelling(checker, text)\n",
    "    # Filter sentence based on whether the reference is not a proper sentence\n",
    "    reference = m2_to_json.clean_text(reference, is_ref=True)\n",
    "    reference = m2_to_json.remove_parenthetical_text(reference)\n",
    "    if only_proper_sent and not m2_to_json.check_proper_sent(reference):\n",
    "        return \"Improper Sentence\"\n",
    "    if text != reference:\n",
    "        # Filter sentence based on the mean similarity between the original and reference sentences\n",
    "        sim_score = m2_to_json.similar_ratio(text, reference)\n",
    "        if sim_score < min_sim:\n",
    "            return \"Source-Reference Similarity\"\n",
    "    return {\"text\": text, \"references\": [reference]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2727c564-a87e-450c-aa16-d97e9dce1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = r\"../data/raw/synthetic/a1/a1_train_incorr_sentences.txt\"\n",
    "trg_path = r\"../data/raw/synthetic/a1/a1_train_corr_sentences.txt\"\n",
    "train_json_path = r\"../data/processed/synthetic/data.json\"\n",
    "dev_json_path = r\"../data/processed/synthetic/dev.json\"\n",
    "min_len = 5\n",
    "max_len = 50\n",
    "min_sim = 0.8\n",
    "only_proper_sent = True\n",
    "spell_check = False\n",
    "remove_ellipsis = True\n",
    "data_limit = 2_000_000\n",
    "dev_percent = 0.02\n",
    "dev_data_num = round(dev_percent*data_limit)\n",
    "\n",
    "json_dir = os.path.dirname(train_json_path)\n",
    "os.makedirs(json_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc96154-6f7f-4951-b869-f5cf04521194",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 0\n",
    "json_data = []\n",
    "stats = defaultdict(int)\n",
    "checker = Speller(lang=\"en\", fast=False, threshold=0)\n",
    "for (incorrect_sent, correct_sent) in tqdm(read_parallel(src_path, trg_path), desc=\"Processing\", unit_scale=True):\n",
    "    result = process_sent(incorrect_sent, correct_sent, checker, min_len, max_len, min_sim, only_proper_sent, spell_check)\n",
    "    if isinstance(result, dict):\n",
    "        json_data.append(result)\n",
    "        num_data += 1\n",
    "        if num_data >= data_limit:\n",
    "            break\n",
    "    else:\n",
    "        stats[result] += 1\n",
    "print(f\"Number of sentences: {len(json_data)}\")\n",
    "print(\"Report of filtered sentences.\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key:>30}: {value}\")\n",
    "    \n",
    "\n",
    "params = {\n",
    "    \"min_len\": min_len,\n",
    "    \"max_len\": max_len,\n",
    "    \"min_sim\": min_sim,\n",
    "    \"spell_check\": spell_check,\n",
    "    \"remove_ellipsis\": remove_ellipsis,\n",
    "    \"only_proper_sent\": only_proper_sent,\n",
    "}\n",
    "write_json(os.path.join(json_dir, \"params.json\"), params)\n",
    "write_json(os.path.join(json_dir, \"metadata.json\"), stats)\n",
    "write_json(train_json_path, json_data[:-dev_data_num])\n",
    "write_json(dev_json_path, json_data[-dev_data_num:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl-gec",
   "language": "python",
   "name": "drl-gec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
