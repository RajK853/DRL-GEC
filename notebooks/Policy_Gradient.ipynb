{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beb33fda-5655-4da4-a91f-b184a5fbc47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from nltk.translate.gleu_score import sentence_gleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a22b63-5b2e-4435-bcd0-afc226226297",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a672e0a-5dba-44a9-a17a-b2be81831902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rajk/Machine_Learning/DRL-GEC\n",
      "/home/rajk/Machine_Learning/DRL-GEC/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import src.envs\n",
    "from src.planning import search_best_actions\n",
    "from src.models.seq2labels import PretrainedEncoder, Seq2Labels\n",
    "from src.utils import remove_ansi, iterative_prediction, load_json, write_json, discount_cumsum, freeze_params, scale\n",
    "%cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3efedd5e-beb3-4d49-90af-0689b133b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e8527e3-74e6-49bf-b43e-3e3696e746df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def select_action(policy, state, reference, mask_generator, explore=False):\n",
    "    [logits] = policy([state])\n",
    "    dist = Categorical(logits=logits)\n",
    "    action_np = search_best_actions(policy, state, mask_generator, reference, explore=explore)\n",
    "    action = torch.from_numpy(action_np).to(logits.device)\n",
    "    log_pi = dist.log_prob(action)\n",
    "    return action_np, log_pi, dist.entropy()\n",
    "\n",
    "def get_log_pi(policy, state, action):\n",
    "    [logits] = policy([state])\n",
    "    dist = Categorical(logits=logits)\n",
    "    action = torch.from_numpy(action).to(logits.device)\n",
    "    log_pi = dist.log_prob(action)\n",
    "    return log_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "007aee43-ac6c-4063-bddb-ede6ed385a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluator(data_path, label_vocab, num_iterations=10):\n",
    "    json_data = load_json(data_path)\n",
    "    src_ref = ((data_dict[\"text\"], data_dict[\"references\"]) for data_dict in json_data) \n",
    "    sources, references = zip(*src_ref)\n",
    "    print(f\"Number of evaluation examples: {len(sources)}\")\n",
    "    del json_data\n",
    "    \n",
    "    def eval_func(policy):\n",
    "        policy.eval()\n",
    "        predictions = iterative_prediction(policy, label_vocab, sources, num_iter=num_iterations, insert_start=True, verbose=False)\n",
    "        score = np.mean([sentence_gleu(refs, pred) for refs, pred in zip(references, predictions)])\n",
    "        policy.train()\n",
    "        return score\n",
    "    \n",
    "    return eval_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93138e1e-97d4-4f37-80bf-63df9b8d3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(pbar, optim, grad_scaler, policy, state_action_batch, return_batch, batch_size=32, focal_alpha=0.25, focal_gamma=2.0, gamma=0.99):\n",
    "    num_items = len(return_batch)\n",
    "    accumulation_size = math.ceil(num_items/batch_size)\n",
    "    # Set up the progress bar\n",
    "    pbar.reset()\n",
    "    pbar.total = num_items\n",
    "    # return_batch = scale(return_batch)\n",
    "    return_batch = torch.tensor(return_batch, device=device)\n",
    "    losses = []\n",
    "    optim.zero_grad()\n",
    "    for i in range(0, num_items, batch_size):\n",
    "        focal_log_pis = []\n",
    "        for (state, action) in state_action_batch[i:i+batch_size]:                                      # Obtain log_pi(state, action) for the batch\n",
    "            log_pi = get_log_pi(policy, state, action)\n",
    "            focal_coef = focal_alpha*(1-log_pi.exp()).pow(focal_gamma)\n",
    "            focal_log_pi = (focal_coef*log_pi).sum()                      # Sum log_probs over tokens               \n",
    "            focal_log_pis.append(focal_log_pi)\n",
    "        focal_log_pis = torch.stack(focal_log_pis)\n",
    "        pi_loss = -(focal_log_pis*return_batch[i:i+batch_size]).mean()\n",
    "        grad_scaler.scale(pi_loss/accumulation_size).backward()\n",
    "        losses.append(pi_loss.item())\n",
    "        pbar.update(len(return_batch[i:i+batch_size]))\n",
    "    grad_scaler.step(optim)\n",
    "    grad_scaler.update()\n",
    "    pbar.refresh()\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04042a3f-a9d1-4761-8497-7e65b06b985e",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97011ba3-ffe0-4c45-818c-1cabeb05f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm parameters\n",
    "eps = 0.9\n",
    "min_eps = 0.01\n",
    "eps_decay = 1 # 0.999995\n",
    "gamma = 0.99\n",
    "focal_alpha = 1.0\n",
    "focal_gamma = 2.0\n",
    "# Training parameters\n",
    "cold_lr = 1e-3\n",
    "warm_lr = 1e-5\n",
    "lr = cold_lr\n",
    "batch_size = 32\n",
    "update_interval = 1000\n",
    "num_unfreeze_layers = 0            # Unfreeze only thhe last 98 layers\n",
    "dropout = 0.1\n",
    "weight_decay = 0.0\n",
    "episodes = 100_000\n",
    "cold_episodes = 0 # 100_000\n",
    "# Evaluation paramters\n",
    "eval_max_iter = 5\n",
    "evaluate_interval = 5000\n",
    "record_output_interval = 10\n",
    "dev_data_path = r\"../data/processed/wi+locness/dev_filtered.json\"\n",
    "model_path = \"sl_logs/pretrain_synthetic_23:10:2022_18:26/model-best.pt\"\n",
    "train_type = \"pretrain\" if model_path is None else \"finetune\"\n",
    "current_datetime = datetime.now().strftime(\"%d_%m_%Y_%H:%M\")\n",
    "log_dir = os.path.join(\"pg_logs\", f\"{train_type}_rl_{current_datetime}\")\n",
    "env_kwargs = {\n",
    "    \"id\": \"gec_lev_dist-v0\",\n",
    "    \"datasets\": [\"wi+locness\"],                  # Datasets to load\n",
    "    \"correct_examples_percent\": [1.0],           # Percentage of correct sentences to load\n",
    "    \"repeat\": 1,                                 # Number of repetation of each sentence in ``\n",
    "    \"repeat_interval\": 1000,                     # Interval to use for repetition\n",
    "    \"consecutive\": False,                        # If the repetition should have consecutive or random distribution\n",
    "}\n",
    "meta_data = {\n",
    "    \"base_model\": model_path,\n",
    "    \"env_config\": env_kwargs,\n",
    "    \"description\": \"\"\"\n",
    "    Finetune the FL pretrained model. \n",
    "    Don't Detach the FL coefficient: alpha*(1-p)^gamma\n",
    "    Disable FL Coef.\n",
    "    Dropout enabled.\n",
    "    With correct sentences.\n",
    "    Reward func = lev_dist(current) - lev_dist(prev); negative reward for false negatives; \n",
    "    positive reward for success; episodes ends when all keep predicted for correct sentence\n",
    "    Don't shuffle batches-\n",
    "    Repeat sentence 1 times in interval 1000 episodes.\n",
    "    New planning: explore = random sample from candidate. exploit = weighted sample from candidate.\n",
    "    Explore-Exploit determined per episode.\n",
    "    No rescaling and Negative rewards. +1 for success.\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba8b9f-7c68-42d6-8a2d-1f5a3bb4e898",
   "metadata": {},
   "source": [
    "# Load environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ce26c80-923b-4c86-a5d2-279ce6bb593b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of data in wi+locness: 24932\n",
      "Number of data without correct sentences: 24932\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(new_step_api=True, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aece764-53dd-45e1-8972-1387200f9010",
   "metadata": {},
   "source": [
    "# Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eda32f0a-611a-4420-a32d-ae7e141d98d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evaluation examples: 3343\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\"\n",
    "tokenizer_config = {\"use_fast\": True}\n",
    "transformer_config = {\n",
    "    \"output_attentions\": False,\n",
    "    # \"hidden_dropout_prob\": 0.0,\n",
    "    # \"attention_probs_dropout_prob\": 0.0,\n",
    "}\n",
    "encoder = PretrainedEncoder(model_name, tokenizer_config, transformer_config).to(device)\n",
    "policy = Seq2Labels(encoder_model=encoder, num_labels=env.action_space.n, dropout=dropout).to(device)\n",
    "if model_path:\n",
    "    policy.load_state_dict(torch.load(model_path))\n",
    "optim = torch.optim.Adam(policy.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "grad_scaler = torch.cuda.amp.GradScaler()\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "evaluator = get_evaluator(dev_data_path, env.labels, eval_max_iter)\n",
    "write_json(os.path.join(log_dir, \"meta.json\"), meta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b15645f-54a0-4585-ad37-b13ab102ba87",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cb6e569-e47a-478b-8d31-4c9a63bba9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frozen parameters: 199/199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c25ffc42d14122a40410521a968022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Episodes:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frozen parameters: 0/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:133: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method should be an int or np.int64, actual type: <class 'list'>\u001b[0m\n",
      "  logger.warn(f\"{pre} should be an int or np.int64, actual type: {type(obs)}\")\n",
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:133: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method should be an int or np.int64, actual type: <class 'list'>\u001b[0m\n",
      "  logger.warn(f\"{pre} should be an int or np.int64, actual type: {type(obs)}\")\n",
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/core.py:57: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n",
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:297: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9863d98c683f450dbb876c91d70fd8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating Policy: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy.train()\n",
    "# Freeze encoder weights\n",
    "freeze_params(policy.encoder, requires_grad=False, optim=optim, lr=lr)\n",
    "# Log hyperparameters\n",
    "writer.add_scalar(\"hyperparameters/gamma\", gamma, 0)\n",
    "writer.add_scalar(\"hyperparameters/dropout\", dropout, 0)\n",
    "writer.add_scalar(\"hyperparameters/batch_size\", batch_size, 0)\n",
    "writer.add_scalar(\"hyperparameters/focal_alpha\", focal_alpha, 0)\n",
    "writer.add_scalar(\"hyperparameters/focal_gamma\", focal_gamma, 0)\n",
    "writer.add_scalar(\"hyperparameters/cold_episodes\", cold_episodes, 0)\n",
    "writer.add_scalar(\"hyperparameters/update_interval\", update_interval, 0)\n",
    "# Variables for training progress bars\n",
    "policy_pbar = None\n",
    "\n",
    "return_batch = []\n",
    "state_action_batch = []\n",
    "mask_generator = env.mask_generator\n",
    "max_eval_score = 0\n",
    "eval_score = evaluator(policy)\n",
    "writer.add_scalar(\"rl/eval_score\", eval_score, 1)\n",
    "for episode in tqdm(range(1, episodes+1), desc=\"Training Episodes\", total=episodes):\n",
    "    if episode-1 == cold_episodes:                # When cold epoch ends, update learning rate and unfreeze certain encoder layers \n",
    "        lr = warm_lr\n",
    "        freeze_params(policy.encoder, requires_grad=True, num_layers=num_unfreeze_layers, optim=optim, lr=lr)\n",
    "    rewards = []\n",
    "    log_pis = []\n",
    "    entropies = []\n",
    "    token_lens = []\n",
    "    done = False\n",
    "    init_state = state = env.reset()\n",
    "    reference = env.current_reference\n",
    "    explore = np.random.uniform() < eps\n",
    "    with torch.cuda.amp.autocast():\n",
    "        while not done:\n",
    "            action, log_pi, entropy = select_action(policy, state, reference, mask_generator, explore=explore)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            # Save timestep data\n",
    "            rewards.append(reward)\n",
    "            log_pis.append(log_pi)\n",
    "            entropies.append(entropy)\n",
    "            token_lens.append(len(next_state))\n",
    "            state_action_batch.append((state, action))\n",
    "            state = next_state\n",
    "    # Compute returns\n",
    "    returns = discount_cumsum(rewards, discount=gamma)\n",
    "    return_batch.extend(returns)\n",
    "    eps = max(eps*eps_decay, min_eps)\n",
    "    # Train the model\n",
    "    if (episode % update_interval) == 0:\n",
    "        if policy_pbar is None:\n",
    "            policy_pbar = tqdm(desc=\"Updating Policy\")\n",
    "        loss = train(policy_pbar, optim, grad_scaler, policy, state_action_batch, return_batch, batch_size=batch_size, focal_alpha=focal_alpha, focal_gamma=focal_gamma, gamma=gamma)\n",
    "        writer.add_scalar(\"rl/mean_loss\", loss, episode)\n",
    "        return_batch = []\n",
    "        state_action_batch = []\n",
    "        torch.cuda.empty_cache()\n",
    "    # Log the episode output to the tensorboard\n",
    "    if (episode % record_output_interval) == 0:\n",
    "        render_output = \"  \\n\".join(remove_ansi(out) for out in env.render())\n",
    "        writer.add_text(\"rl/output\", render_output, episode)\n",
    "    # Evaluate the model\n",
    "    if (episode % evaluate_interval) == 0:\n",
    "        eval_score = evaluator(policy)\n",
    "        if eval_score >= max_eval_score:\n",
    "            torch.save(policy.state_dict(), os.path.join(log_dir, \"model-best.pt\"))\n",
    "            max_eval_score = eval_score\n",
    "        writer.add_scalar(\"rl/eval_score\", eval_score, episode)\n",
    "    # Log scalar episode results\n",
    "    rewards = np.array(rewards)\n",
    "    writer.add_scalar(\"rl/lr\", lr, episode)\n",
    "    writer.add_scalar(\"rl/eps\", eps, episode)\n",
    "    writer.add_scalar(\"rl/explore\", explore, episode)\n",
    "    writer.add_scalar(\"rl/episode_length\", len(rewards), episode)\n",
    "    writer.add_scalar(\"rl/episode_reward_last\", rewards[-1], episode)\n",
    "    writer.add_scalar(\"rl/episode_reward_total\", sum(rewards), episode)\n",
    "    # writer.add_scalar(\"rl/episode_reward_delta\", rewards[-1]-rewards[0], episode)\n",
    "    writer.add_scalar(\"rl/token_length_delta_ratio\", (len(state)-len(init_state))/len(init_state), episode)\n",
    "    # Log histogram episode results\n",
    "    writer.add_histogram(\"rl/episode_reward\", rewards, episode)\n",
    "    writer.add_histogram(\"rl/episode_returns\", returns, episode)\n",
    "    writer.add_histogram(\"rl/episode_log_pi\", torch.cat(log_pis), episode)\n",
    "    writer.add_histogram(\"rl/episode_entropy\", torch.cat(entropies), episode)\n",
    "    writer.add_histogram(\"rl/episode_token_length\", np.array(token_lens), episode)\n",
    "    writer.add_histogram(\"rl/episode_returns_normalized\", np.array(scale(returns)), episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917beb2-bc40-477e-b3e7-9402ceef4692",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "065d550f-56a6-4eaf-bb21-649a5d718a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), os.path.join(log_dir, \"model-last.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa3edbc-25eb-4a35-be1a-0ca26befc9dd",
   "metadata": {},
   "source": [
    "policy.load_state_dict(torch.load(os.path.join(log_dir, \"model-best.pt\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd3700-01af-472b-a33c-93bbb23e62dd",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8454cfc-e75e-4b1b-8564-91f2c811d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_action(policy, state):\n",
    "    [logits] = policy([state])\n",
    "    v, i = logits.topk(5)\n",
    "    v = v.cpu().numpy()\n",
    "    i = i.cpu().numpy()\n",
    "    dist = Categorical(logits=logits)\n",
    "    probs = dist.probs\n",
    "    entropy = dist.entropy().cpu().numpy()\n",
    "    for a, e, lp in zip(state, entropy, zip(env.labels[i], v)):\n",
    "        print(f\"{e:4f}, {a:15}\", \" -- \".join(f\"{l} [{p:5.2f}]\" for (l, p) in zip(*lp)))\n",
    "    print()\n",
    "    action = logits.argmax(axis=-1)\n",
    "    # action = Categorical(logits=logits).sample()\n",
    "    return action.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbf4c3f7-91f0-4340-b30a-a21efe93fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = policy.eval()          # Set poliy to eval mode to disable dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51f01563-aea0-41d5-98c2-0e63f8cec77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of data in wi+locness: 24932\n",
      "Number of data without correct sentences: 15586\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"wi_locness_gec_lev_dist-v0\", new_step_api=True, correct_examples_percent=[0.0], repeat=1, repeat_interval=5000, consecutive=False, min_num_refs=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f31a90a3-3021-4443-81af-e857a1f81e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# References\n",
      "['$START', 'In', 'my', 'opinion', ',', 'I', 'am', 'the', 'perfect', 'candidate', 'for', 'this', 'vacancy', '.']\n",
      "\n",
      "0.004923, $START          $KEEP [14.93] -- $APPEND_But [ 6.21] -- $APPEND_, [ 5.60] -- $APPEND_And [ 4.12] -- $APPEND_The [ 3.98]\n",
      "0.208553, In              $KEEP [11.09] -- $DELETE [ 6.85] -- $APPEND_, [ 6.34] -- $REPLACE_On [ 4.52] -- $TRANSFORM_CASE_CAPITAL [ 3.95]\n",
      "0.119008, my              $KEEP [10.88] -- $DELETE [ 5.85] -- $APPEND_, [ 5.50] -- $APPEND_own [ 4.45] -- $APPEND_. [ 3.20]\n",
      "0.305871, opinion         $APPEND_, [13.61] -- $KEEP [11.22] -- $REPLACE_, [ 6.68] -- $APPEND_. [ 5.68] -- $APPEND_- [ 5.59]\n",
      "0.050036, I               $KEEP [11.96] -- $APPEND_, [ 5.83] -- $DELETE [ 5.61] -- $APPEND_am [ 4.05] -- $APPEND_I [ 4.04]\n",
      "0.095194, am              $KEEP [11.08] -- $REPLACE_was [ 5.40] -- $APPEND_, [ 4.53] -- $DELETE [ 4.36] -- $REPLACE_'m [ 3.80]\n",
      "0.048151, the             $KEEP [13.07] -- $REPLACE_a [ 7.87] -- $APPEND_a [ 5.27] -- $APPEND_the [ 5.10] -- $DELETE [ 4.81]\n",
      "0.014372, perfect         $KEEP [12.89] -- $APPEND_\" [ 3.93] -- $REPLACE_perfect [ 3.86] -- $APPEND_, [ 3.65] -- $APPEND_a [ 3.64]\n",
      "0.011402, candidate       $KEEP [13.60] -- $APPEND_for [ 6.16] -- $APPEND_, [ 4.95] -- $APPEND_to [ 3.63] -- $TRANSFORM_AGREEMENT_PLURAL [ 3.57]\n",
      "0.038582, for             $KEEP [11.66] -- $REPLACE_to [ 4.69] -- $APPEND_the [ 4.22] -- $REPLACE_for [ 3.74] -- $DELETE [ 3.25]\n",
      "0.046927, this            $KEEP [11.49] -- $REPLACE_the [ 5.02] -- $REPLACE_this [ 3.84] -- $REPLACE_these [ 3.54] -- $DELETE [ 3.22]\n",
      "0.041632, vacancy         $KEEP [11.12] -- $TRANSFORM_AGREEMENT_PLURAL [ 3.52] -- $APPEND_, [ 3.06] -- $APPEND_yesterday [ 2.26] -- $DELETE [ 2.15]\n",
      "0.004686, .               $KEEP [14.54] -- $APPEND_\" [ 6.27] -- $REPLACE_? [ 4.38] -- $APPEND_The [ 4.32] -- $REPLACE_! [ 3.99]\n",
      "\n",
      "\u001b[37;1mTimestep:\u001b[0m 0  \n",
      "\u001b[37;1mRewards:\u001b[0m 0.000  \n",
      "\u001b[37;1mSource:\u001b[0m $START In my opinion I am the perfect candidate for this vacancy .  \n",
      "\u001b[37;1mOutput:\u001b[0m $START In my opinion I am the perfect candidate for this vacancy .  \n",
      "\n",
      "\u001b[37;1mTimestep:\u001b[0m 1  \n",
      "\u001b[37;1mRewards:\u001b[0m 0.035  \n",
      "\u001b[37;1mSource:\u001b[0m $START In my \u001b[32;1mopinion\u001b[0m [\u001b[31;1m$APPEND_,\u001b[0m] I am the perfect candidate for this vacancy .  \n",
      "\u001b[37;1mOutput:\u001b[0m $START In my opinion , I am the perfect candidate for this vacancy .  \n",
      "\n",
      "0.004204, $START          $KEEP [15.16] -- $APPEND_But [ 6.31] -- $APPEND_, [ 5.76] -- $APPEND_And [ 4.20] -- $APPEND_The [ 3.84]\n",
      "0.138974, In              $KEEP [11.42] -- $DELETE [ 6.62] -- $APPEND_, [ 6.18] -- $REPLACE_In [ 4.52] -- $REPLACE_On [ 4.12]\n",
      "0.060096, my              $KEEP [11.51] -- $DELETE [ 5.41] -- $APPEND_, [ 5.06] -- $APPEND_own [ 4.71] -- $REPLACE_my [ 3.61]\n",
      "0.106052, opinion         $KEEP [10.83] -- $APPEND_, [ 5.17] -- $DELETE [ 4.84] -- $TRANSFORM_AGREEMENT_PLURAL [ 3.73] -- $APPEND_. [ 2.97]\n",
      "0.024077, ,               $KEEP [13.48] -- $APPEND_, [ 7.23] -- $DELETE [ 5.65] -- $APPEND_the [ 5.11] -- $APPEND_\" [ 3.05]\n",
      "0.024572, I               $KEEP [12.54] -- $DELETE [ 5.52] -- $APPEND_, [ 4.51] -- $APPEND_am [ 4.43] -- $APPEND_I [ 3.45]\n",
      "0.087900, am              $KEEP [11.16] -- $REPLACE_was [ 5.41] -- $DELETE [ 4.35] -- $APPEND_, [ 4.10] -- $REPLACE_is [ 3.61]\n",
      "0.049378, the             $KEEP [13.04] -- $REPLACE_a [ 7.87] -- $APPEND_a [ 5.23] -- $APPEND_the [ 4.97] -- $DELETE [ 4.79]\n",
      "0.013822, perfect         $KEEP [12.93] -- $APPEND_\" [ 4.01] -- $REPLACE_perfect [ 3.89] -- $APPEND_, [ 3.57] -- $APPEND_a [ 3.50]\n",
      "0.012000, candidate       $KEEP [13.59] -- $APPEND_for [ 6.29] -- $APPEND_, [ 4.79] -- $APPEND_to [ 3.74] -- $TRANSFORM_AGREEMENT_PLURAL [ 3.53]\n",
      "0.037169, for             $KEEP [11.68] -- $REPLACE_to [ 4.63] -- $APPEND_the [ 4.15] -- $REPLACE_for [ 3.73] -- $APPEND_for [ 3.24]\n",
      "0.044269, this            $KEEP [11.55] -- $REPLACE_the [ 4.96] -- $REPLACE_this [ 3.93] -- $REPLACE_these [ 3.62] -- $DELETE [ 3.18]\n",
      "0.039928, vacancy         $KEEP [11.14] -- $TRANSFORM_AGREEMENT_PLURAL [ 3.50] -- $APPEND_, [ 2.57] -- $APPEND_yesterday [ 2.21] -- $DELETE [ 2.14]\n",
      "0.004105, .               $KEEP [14.73] -- $APPEND_\" [ 6.33] -- $REPLACE_? [ 4.49] -- $APPEND_The [ 4.41] -- $REPLACE_! [ 4.11]\n",
      "\n",
      "\u001b[37;1mTimestep:\u001b[0m \u001b[32;1m2\u001b[0m  \n",
      "\u001b[37;1mRewards:\u001b[0m 1.000  \n",
      "\u001b[37;1mSource:\u001b[0m $START In my opinion , I am the perfect candidate for this vacancy .  \n",
      "\u001b[37;1mOutput:\u001b[0m $START In my opinion , I am the perfect candidate for this vacancy .  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(\"# References\")\n",
    "for ref in env.reference_tokens_list:\n",
    "    print(ref)\n",
    "print()\n",
    "done = False\n",
    "while not done:\n",
    "    action = greedy_action(policy, state)\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    state = next_state\n",
    "    outputs = env.render()\n",
    "    for o in outputs:\n",
    "        print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b5610f-e29f-45fe-b9b2-b7497e6f6e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75b268-8f2e-4ee0-bc24-bbec9c4d736b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl-gec",
   "language": "python",
   "name": "drl-gec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
