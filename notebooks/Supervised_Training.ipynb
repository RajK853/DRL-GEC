{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c01e920-068e-46e2-a6c8-eecadfd63987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd9fa98-d239-422c-9bcc-4f384f13cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2edf823-39a1-43b6-ac3e-75942f368fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rajk/Machine_Learning/DRL-GEC\n",
      "/home/rajk/Machine_Learning/DRL-GEC/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "from src import envs\n",
    "from src.sl.losses import FocalLoss\n",
    "from src.sl.dataset import GECDataset\n",
    "from src.sl.utils import process_data, collate_func\n",
    "from src.utils import load_text, write_json, freeze_params\n",
    "from src.models.seq2labels import PretrainedEncoder, Seq2Labels\n",
    "%cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a87287-3b10-412a-a3db-f9fe8db5b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f1639-0284-467f-970d-bd9c37483731",
   "metadata": {},
   "source": [
    "# Local Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e2390b2-9339-49bf-b835-3a4d5c925cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.cuda.amp.autocast()\n",
    "def evaluate(model, batch, criterion):\n",
    "    masks = torch.from_numpy(batch[\"masks\"]).to(device)\n",
    "    labels = torch.from_numpy(batch['labels']).to(device)\n",
    "    logits = model(tokens=batch['tokens'])\n",
    "    batch_size, seq_size, labels_size = logits.shape\n",
    "    loss = criterion(logits.view(-1, labels_size), labels.view(-1))\n",
    "    loss = loss.view(batch_size, seq_size).sum(dim=-1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef892698-34d8-4e41-9d13-8c8cd752a8e3",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5941764f-acfa-4d42-93de-5a813a4b23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_lr = 1e-3\n",
    "warm_lr = 1e-5\n",
    "lr = cold_lr\n",
    "dropout = 0.1\n",
    "num_epochs = 20\n",
    "cold_epochs = 2\n",
    "patience = 3\n",
    "batch_size = 64\n",
    "accumulation_size = 4\n",
    "weight_decay = 0\n",
    "data_limit = 500_000\n",
    "keep_corrects = False\n",
    "num_unfreeze_layers = 0\n",
    "train_datasets = [\"synthetic\"]\n",
    "val_datasets = [\"synthetic\"]\n",
    "current_datetime = datetime.now().strftime(\"%d:%m:%Y_%H:%M\")\n",
    "model_path = None\n",
    "train_type = \"pretrain\" if model_path is None else \"finetune\"\n",
    "log_dir = os.path.join(\"sl_logs\", f\"{train_type}_{'-'.join(train_datasets)}_{current_datetime}\")\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "meta_data = {\n",
    "    \"description\": \"\"\"\n",
    "    Pretrain on 500k Synthetic data.\n",
    "    Use Focal Loss.\n",
    "    Use Unknown label\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b1be0-19f0-4dcc-a626-f09b038454d3",
   "metadata": {},
   "source": [
    "# Load label vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f400ab5-4c30-4293-a25c-d5ade22bc887",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = \"../data/vocabs/labels.txt\"\n",
    "label_vocab = load_text(label_path)\n",
    "label2index = {label:i for i, label in enumerate(label_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650cbc6-7d5a-4f3d-a98e-e069507633a8",
   "metadata": {},
   "source": [
    "# Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bf1337b-ed98-4cf7-81e7-88e4de6fa94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ff7bb04d0641009dd2bc090e0e89f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncating amount of data from 3290450 to 500000\n",
      "Total number of sentences: 500000\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "for dataset in tqdm(train_datasets, desc=\"Loading datasets\", total=len(train_datasets)):\n",
    "    data_path = f\"../data/processed/{dataset}/data.gector\"\n",
    "    train_data.extend(load_text(data_path))\n",
    "if (data_limit > 0) and (len(train_data) > data_limit):\n",
    "    print(f\"Truncating amount of data from {len(train_data)} to {data_limit}\")\n",
    "    train_data = train_data[:data_limit]\n",
    "print(f\"Total number of sentences: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa24a1f9-07ce-4510-aac9-0513ffed1b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b704768be3b3413dbe908efc423c7117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 40000\n"
     ]
    }
   ],
   "source": [
    "dev_data = []\n",
    "for dataset in tqdm(val_datasets, desc=\"Loading datasets\", total=len(val_datasets)):\n",
    "    data_path = f\"../data/processed/{dataset}/dev.gector\"\n",
    "    dev_data.extend(load_text(data_path))\n",
    "print(f\"Total number of sentences: {len(dev_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f4291-ca79-45be-a1cc-4d447d4bc0f7",
   "metadata": {},
   "source": [
    "# Extract tokens and labels from the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d8efd7c-a3e4-481b-82af-36780efdd260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a3515dd9324ec9a761d0aa93dfd965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data:   0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of data after filtering: 470654\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278188ae9b7e46bb8b451be5f894eab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data:   0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of data after filtering: 40000\n"
     ]
    }
   ],
   "source": [
    "train_tokens, train_labels = process_data(train_data, label_vocab, keep_corrects=keep_corrects)\n",
    "dev_tokens, dev_labels = process_data(dev_data, label_vocab, keep_corrects=True)\n",
    "train_dataset = GECDataset(train_tokens, train_labels, label2index)\n",
    "dev_dataset = GECDataset(dev_tokens, dev_labels, label2index)\n",
    "train_loader = DataLoader(train_dataset, batch_size=int(batch_size/accumulation_size), shuffle=True, num_workers=4, collate_fn=collate_func)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f4a8be0-5db7-480a-be00-6834e5cf8c16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\"\n",
    "tokenizer_config = {\"use_fast\": True}\n",
    "transformer_config = {\"output_attentions\": False}\n",
    "\n",
    "encoder = PretrainedEncoder(model_name, tokenizer_config, transformer_config).to(device)\n",
    "model = Seq2Labels(encoder_model=encoder, num_labels=len(label_vocab), dropout=dropout).to(device)\n",
    "if model_path:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0, reduction=\"none\")\n",
    "# criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "grad_scaler = torch.cuda.amp.GradScaler()\n",
    "write_json(os.path.join(log_dir, \"meta.json\"), meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39dff37-8de5-41fa-971c-55d2294685f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frozen parameters: 197/197\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5821857d56514851a7b4f38a9f85d07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be41a9cd4a74e49aeaeb8782b4aba99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/29416 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d6a2188a70430b8dfd9bc55d19cf1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/29416 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frozen parameters: 0/197\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7041f9a7838a489bb55ed07bae32428e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/29416 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955230b86de14a989abb6bbf6721d4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/29416 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train()\n",
    "optim.zero_grad()\n",
    "N = len(train_loader)\n",
    "freeze_params(model.encoder, requires_grad=False)    # Freeze encoder model\n",
    "# Log hyperparameters\n",
    "writer.add_scalar(\"hyperparameters/dropout\", dropout, 0)\n",
    "writer.add_scalar(\"hyperparameters/patience\", patience, 0)\n",
    "writer.add_scalar(\"hyperparameters/batch_size\", batch_size, 0)\n",
    "writer.add_scalar(\"hyperparameters/cold_epochs\", cold_epochs, 0)\n",
    "writer.add_scalar(\"hyperparameters/weight_decay\", weight_decay, 0)\n",
    "writer.add_scalar(\"hyperparameters/keep_corrects\", int(keep_corrects), 0)\n",
    "writer.add_scalar(\"hyperparameters/accumulation_size\", accumulation_size, 0)\n",
    "writer.add_scalar(\"hyperparameters/num_unfreeze_layers\", num_unfreeze_layers, 0)\n",
    "writer.add_scalar(\"hyperparameters/uses_CE_loss\", int(isinstance(criterion, nn.CrossEntropyLoss)), 0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    dev_losses = [evaluate(model, batch, criterion) for batch in dev_loader]\n",
    "    dev_loss = torch.cat(dev_losses).mean()\n",
    "    writer.add_scalar(\"sl/validation_loss\", dev_loss, 0)\n",
    "\n",
    "epochs_since_improvement = 0\n",
    "best_dev_score = dev_loss\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training\", total=num_epochs):\n",
    "    if epoch == cold_epochs:                                                                           # End of the cold epochs\n",
    "        lr = warm_lr\n",
    "        freeze_params(model.encoder, requires_grad=True, num_layers=num_unfreeze_layers, optim=optim, lr=lr)               # Unfreeze encoder model\n",
    "    \n",
    "    step_offset = epoch*N\n",
    "    for i, batch in tqdm(enumerate(train_loader), desc=f\"Epoch {epoch+1}\", total=len(train_loader)):\n",
    "        loss = evaluate(model, batch, criterion)\n",
    "        loss = loss.mean()\n",
    "        grad_scaler.scale(loss/accumulation_size).backward()\n",
    "        if ((i+1) % accumulation_size) == 0:\n",
    "            grad_scaler.step(optim)\n",
    "            grad_scaler.update()\n",
    "            optim.zero_grad()\n",
    "            torch.cuda.empty_cache()\n",
    "        writer.add_scalar(\"sl/lr\", lr, step_offset + i)\n",
    "        writer.add_scalar(\"sl/train_loss\", loss, step_offset + i)\n",
    "    with torch.no_grad():\n",
    "        dev_losses = [evaluate(model, batch, criterion) for batch in dev_loader]\n",
    "    dev_loss = torch.cat(dev_losses).mean()\n",
    "    writer.add_scalar(\"sl/validation_loss\", dev_loss, step_offset + i)\n",
    "    if dev_loss <= best_dev_score:\n",
    "        best_dev_score = dev_loss\n",
    "        epochs_since_improvement = 0\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, \"model-best.pt\"))     # Save best model \n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eefa210-1b3e-428a-9bc8-4d4863a2c162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(log_dir, \"model-last.pt\"))                 # Save last model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0b601-3e0e-4bcf-842d-57f45a8d7f9a",
   "metadata": {},
   "source": [
    "model.load_state_dict(torch.load(os.path.join(log_dir, \"model-best.pt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dce900-e82c-47b4-a879-86db15ed7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd004af7-e6ed-4d39-ac76-911a517c1f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"wi_locness_gec-v0\", correct_examples_percent=[0.0], repeat=1, min_num_refs=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e60cd-f01f-48f5-a711-bc395ebd4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "state = env.reset()\n",
    "print(\"# References\")\n",
    "for ref in env.reference_tokens_list:\n",
    "    print(ref)\n",
    "print()\n",
    "done = False\n",
    "while not done:\n",
    "    with torch.no_grad():\n",
    "        [logits] = model([state])\n",
    "        actions = logits.argmax(-1)\n",
    "        actions = actions.cpu().numpy()\n",
    "        v, i = logits.topk(5)\n",
    "        v = v.cpu().numpy()\n",
    "        i = i.cpu().numpy()\n",
    "        for s, lp in zip(state, zip(env.labels[i], v)):\n",
    "            print(f\"{s:15}\", \" --- \".join(f\"{l:9} [{p:5.2f}]\" for (l, p) in zip(*lp)))\n",
    "        print()\n",
    "    next_state, reward, done, info = env.step(actions)\n",
    "    state = next_state\n",
    "    outputs = env.render()\n",
    "    for o in outputs:\n",
    "        print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a8cb1-2b94-49e3-aff9-504ea19cef04",
   "metadata": {},
   "source": [
    "# Close Google Compute Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ab9eb-332f-4759-a501-918bc33f4748",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud compute instances stop drl-gec --zone us-west1-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ab31a-6f54-442b-9c09-6fd1ab01759e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl-gec",
   "language": "python",
   "name": "drl-gec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
