{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c01e920-068e-46e2-a6c8-eecadfd63987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd9fa98-d239-422c-9bcc-4f384f13cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2edf823-39a1-43b6-ac3e-75942f368fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rajk/Machine_Learning/DRL-GEC\n",
      "/home/rajk/Machine_Learning/DRL-GEC/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "from src import envs\n",
    "from src.sl.losses import FocalLoss\n",
    "from src.sl.dataset import GECDataset\n",
    "from src.sl.utils import process_data, collate_func\n",
    "from src.models.seq2labels import PretrainedEncoder, Seq2Labels\n",
    "from src.utils import load_text, write_json, freeze_params, is_gce_instance\n",
    "%cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a87287-3b10-412a-a3db-f9fe8db5b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f1639-0284-467f-970d-bd9c37483731",
   "metadata": {},
   "source": [
    "# Local Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e2390b2-9339-49bf-b835-3a4d5c925cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.cuda.amp.autocast()\n",
    "def evaluate(model, batch, criterion):\n",
    "    masks = torch.from_numpy(batch[\"masks\"]).to(device)\n",
    "    labels = torch.from_numpy(batch['labels']).to(device)\n",
    "    logits = model(tokens=batch['tokens'])\n",
    "    batch_size, seq_size, labels_size = logits.shape\n",
    "    loss = criterion(logits.view(-1, labels_size), labels.view(-1))\n",
    "    loss = loss.view(batch_size, seq_size).sum(dim=-1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef892698-34d8-4e41-9d13-8c8cd752a8e3",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5941764f-acfa-4d42-93de-5a813a4b23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_lr = 1e-3\n",
    "warm_lr = 1e-5\n",
    "lr = cold_lr\n",
    "dropout = 0.1\n",
    "num_epochs = 20\n",
    "cold_epochs = 0\n",
    "patience = 2\n",
    "batch_size = 64\n",
    "accumulation_size = 4\n",
    "num_workers = 4\n",
    "weight_decay = 0\n",
    "data_limit = 500_000\n",
    "keep_corrects = True\n",
    "num_unfreeze_layers = 0\n",
    "train_datasets = [\"wi+locness\"]\n",
    "val_datasets = [\"wi+locness\"]\n",
    "current_datetime = datetime.now().strftime(\"%d:%m:%Y_%H:%M\")\n",
    "model_path = \"sl_logs/pretrain_synthetic_31:10:2022_09:39/model-best.pt\"\n",
    "label_path = \"../data/vocabs/labels.txt\"\n",
    "train_type = \"pretrain\" if model_path is None else \"finetune\"\n",
    "log_dir = os.path.join(\"sl_logs\", f\"{train_type}_{'-'.join(train_datasets)}_{current_datetime}\")\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "meta_data = {\n",
    "    \"description\": \"\"\"\n",
    "    Finetune on WI+Locness data.\n",
    "    Use Focal Loss.\n",
    "    Use Unknown label\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b1be0-19f0-4dcc-a626-f09b038454d3",
   "metadata": {},
   "source": [
    "# Load label vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f400ab5-4c30-4293-a25c-d5ade22bc887",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab = load_text(label_path)\n",
    "label2index = {label:i for i, label in enumerate(label_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650cbc6-7d5a-4f3d-a98e-e069507633a8",
   "metadata": {},
   "source": [
    "# Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bf1337b-ed98-4cf7-81e7-88e4de6fa94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48bcd3335d044aeab9561e283b989ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 26815\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "for dataset in tqdm(train_datasets, desc=\"Loading datasets\", total=len(train_datasets)):\n",
    "    data_path = f\"../data/processed/{dataset}/data.gector\"\n",
    "    train_data.extend(load_text(data_path))\n",
    "if (data_limit > 0) and (len(train_data) > data_limit):\n",
    "    print(f\"Truncating amount of data from {len(train_data)} to {data_limit}\")\n",
    "    train_data = train_data[:data_limit]\n",
    "print(f\"Total number of sentences: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa24a1f9-07ce-4510-aac9-0513ffed1b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f6dbcfd2274283bccd66f2e7b89e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 3590\n"
     ]
    }
   ],
   "source": [
    "val_data = []\n",
    "for dataset in tqdm(val_datasets, desc=\"Loading datasets\", total=len(val_datasets)):\n",
    "    data_path = f\"../data/processed/{dataset}/dev.gector\"\n",
    "    val_data.extend(load_text(data_path))\n",
    "print(f\"Total number of sentences: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f4291-ca79-45be-a1cc-4d447d4bc0f7",
   "metadata": {},
   "source": [
    "# Extract tokens and labels from the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d8efd7c-a3e4-481b-82af-36780efdd260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f775874b657b4ce2a640d438d633bff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data:   0%|          | 0/26815 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of data after filtering: 26815\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78afe70f665e4dfc906405c23d6c03aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data:   0%|          | 0/3590 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of data after filtering: 3590\n"
     ]
    }
   ],
   "source": [
    "train_tokens, train_labels = process_data(train_data, label_vocab, keep_corrects=keep_corrects)\n",
    "val_tokens, val_labels = process_data(val_data, label_vocab, keep_corrects=True)\n",
    "train_dataset = GECDataset(train_tokens, train_labels, label2index)\n",
    "val_dataset = GECDataset(val_tokens, val_labels, label2index)\n",
    "train_loader = DataLoader(train_dataset, batch_size=int(batch_size/accumulation_size), shuffle=True, num_workers=4, collate_fn=collate_func)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f4a8be0-5db7-480a-be00-6834e5cf8c16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\"\n",
    "tokenizer_config = {\"use_fast\": True}\n",
    "transformer_config = {\"output_attentions\": False}\n",
    "\n",
    "encoder = PretrainedEncoder(model_name, tokenizer_config, transformer_config).to(device)\n",
    "model = Seq2Labels(encoder_model=encoder, num_labels=len(label_vocab), dropout=dropout).to(device)\n",
    "if model_path:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0, reduction=\"none\")\n",
    "# criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "grad_scaler = torch.cuda.amp.GradScaler()\n",
    "write_json(os.path.join(log_dir, \"meta.json\"), meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d39dff37-8de5-41fa-971c-55d2294685f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frozen parameters: 197/197\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585b673e3a154005a0693595a43e3951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frozen parameters: 0/197\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d12c694b9b442db0e46f34df327644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120241091ef04621aa6d32ab23a7b5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/1676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454e33a7bb204d27ba3c67527af6d850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/1676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acffa9a8d614dd493431ddfaded4051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/1676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e3424ed5084b20909936d0405fcefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/1676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd09fafd1954afb9235a27f53438d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/1676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a758fe8d0404eaaa95543bc81d78cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/1676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "optim.zero_grad()\n",
    "N = len(train_loader)\n",
    "freeze_params(model.encoder, requires_grad=False)    # Freeze encoder model\n",
    "# Log hyperparameters\n",
    "writer.add_scalar(\"hyperparameters/dropout\", dropout, 0)\n",
    "writer.add_scalar(\"hyperparameters/patience\", patience, 0)\n",
    "writer.add_scalar(\"hyperparameters/batch_size\", batch_size, 0)\n",
    "writer.add_scalar(\"hyperparameters/cold_epochs\", cold_epochs, 0)\n",
    "writer.add_scalar(\"hyperparameters/weight_decay\", weight_decay, 0)\n",
    "writer.add_scalar(\"hyperparameters/keep_corrects\", int(keep_corrects), 0)\n",
    "writer.add_scalar(\"hyperparameters/accumulation_size\", accumulation_size, 0)\n",
    "writer.add_scalar(\"hyperparameters/num_unfreeze_layers\", num_unfreeze_layers, 0)\n",
    "writer.add_scalar(\"hyperparameters/uses_CE_loss\", int(isinstance(criterion, nn.CrossEntropyLoss)), 0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_losses = [evaluate(model, batch, criterion) for batch in val_loader]\n",
    "    val_loss = torch.cat(val_losses).mean()\n",
    "    writer.add_scalar(\"sl/validation_loss\", val_loss, 0)\n",
    "\n",
    "epochs_since_improvement = 0\n",
    "best_val_score = val_loss\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training\", total=num_epochs):\n",
    "    if epoch == cold_epochs:                                                                                  # End of the cold epochs\n",
    "        lr = warm_lr\n",
    "        freeze_params(model.encoder, requires_grad=True, num_layers=num_unfreeze_layers, optim=optim, lr=lr)  # Unfreeze encoder model\n",
    "        if cold_epochs > 0:                                                                                   # Save model after the cold epochs\n",
    "            torch.save(model.state_dict(), os.path.join(log_dir, \"model-cold.pt\"))\n",
    "    \n",
    "    step_offset = epoch*N\n",
    "    for i, batch in tqdm(enumerate(train_loader), desc=f\"Epoch {epoch+1}\", total=len(train_loader)):\n",
    "        loss = evaluate(model, batch, criterion)\n",
    "        loss = loss.mean()\n",
    "        grad_scaler.scale(loss/accumulation_size).backward()\n",
    "        if ((i+1) % accumulation_size) == 0:\n",
    "            grad_scaler.step(optim)\n",
    "            grad_scaler.update()\n",
    "            optim.zero_grad()\n",
    "            torch.cuda.empty_cache()\n",
    "        writer.add_scalar(\"sl/lr\", lr, step_offset + i)\n",
    "        writer.add_scalar(\"sl/train_loss\", loss, step_offset + i)\n",
    "    with torch.no_grad():\n",
    "        val_losses = [evaluate(model, batch, criterion) for batch in val_loader]\n",
    "    val_loss = torch.cat(val_losses).mean()\n",
    "    writer.add_scalar(\"sl/validation_loss\", val_loss, step_offset + i)\n",
    "    if val_loss <= best_val_score:\n",
    "        best_val_score = val_loss\n",
    "        epochs_since_improvement = 0\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, \"model-best.pt\"))     # Save best model \n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eefa210-1b3e-428a-9bc8-4d4863a2c162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(log_dir, \"model-last.pt\"))                 # Save last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34dce900-e82c-47b4-a879-86db15ed7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd004af7-e6ed-4d39-ac76-911a517c1f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of data in wi+locness: 26815\n",
      "Number of data without correct sentences: 17494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"wi_locness_gec_lev_dist-v0\", correct_examples_percent=[0.0], repeat=1, min_num_refs=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "725e60cd-f01f-48f5-a711-bc395ebd4408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# References\n",
      "['$START', 'I', 'believe', 'he', 'is', 'tender', 'and', 'caring', ',', 'but', 'impatient', 'and', 'clumsy', 'at', 'the', 'same', 'time', '.']\n",
      "\n",
      "$START          $KEEP     [ 9.65] --- $DELETE   [ 5.04] --- $APPEND_\" [ 4.41] --- $UNKNOWN  [ 3.69] --- $REPLACE_? [ 3.67]\n",
      "I               $KEEP     [10.52] --- $APPEND_do [ 6.72] --- $APPEND_would [ 5.53] --- $APPEND_also [ 5.30] --- $APPEND_really [ 4.91]\n",
      "believe         $KEEP     [ 9.13] --- $REPLACE_think [ 6.03] --- $APPEND_that [ 5.10] --- $APPEND_, [ 4.26] --- $REPLACE_believe [ 4.06]\n",
      "he              $KEEP     [ 8.90] --- $REPLACE_he [ 5.30] --- $DELETE   [ 4.26] --- $REPLACE_it [ 2.94] --- $REPLACE_him [ 2.87]\n",
      "is              $KEEP     [ 9.02] --- $REPLACE_was [ 6.56] --- $APPEND_a [ 4.64] --- $APPEND_also [ 4.26] --- $DELETE   [ 3.92]\n",
      "tender          $KEEP     [10.83] --- $UNKNOWN  [ 7.88] --- $REPLACE_careful [ 6.85] --- $REPLACE_happy [ 5.14] --- $REPLACE_nice [ 4.93]\n",
      "and             $KEEP     [ 9.51] --- $APPEND_very [ 4.77] --- $APPEND_sometimes [ 4.40] --- $DELETE   [ 4.17] --- $APPEND_a [ 3.73]\n",
      "careful         $KEEP     [ 9.16] --- $UNKNOWN  [ 5.79] --- $DELETE   [ 3.84] --- $REPLACE_careful [ 3.11] --- $MERGE_SPACE [ 2.19]\n",
      ",               $KEEP     [ 8.79] --- $DELETE   [ 6.37] --- $UNKNOWN  [ 3.24] --- $APPEND_, [ 3.00] --- $APPEND_and [ 2.70]\n",
      "but             $KEEP     [ 9.43] --- $APPEND_also [ 7.84] --- $APPEND_sometimes [ 5.81] --- $REPLACE_and [ 5.67] --- $APPEND_is [ 4.82]\n",
      "impatient       $KEEP     [ 9.24] --- $DELETE   [ 5.49] --- $UNKNOWN  [ 4.61] --- $APPEND_, [ 3.50] --- $APPEND_\" [ 1.60]\n",
      "and             $KEEP     [ 9.53] --- $APPEND_sometimes [ 4.85] --- $DELETE   [ 4.68] --- $APPEND_very [ 4.04] --- $APPEND_also [ 3.65]\n",
      "clumsy          $KEEP     [ 9.93] --- $UNKNOWN  [ 6.44] --- $DELETE   [ 5.09] --- $APPEND_, [ 4.37] --- $APPEND_\" [ 3.23]\n",
      "at              $KEEP     [10.73] --- $DELETE   [ 5.82] --- $APPEND_all [ 5.44] --- $REPLACE_in [ 4.56] --- $REPLACE_at [ 4.11]\n",
      "the             $KEEP     [10.82] --- $DELETE   [ 5.71] --- $REPLACE_the [ 4.86] --- $APPEND_the [ 4.68] --- $UNKNOWN  [ 3.36]\n",
      "same            $KEEP     [11.13] --- $DELETE   [ 5.61] --- $APPEND_a [ 3.85] --- $APPEND_the [ 3.46] --- $UNKNOWN  [ 3.20]\n",
      "time            $KEEP     [10.78] --- $UNKNOWN  [ 4.20] --- $REPLACE_time [ 3.95] --- $APPEND_, [ 3.42] --- $DELETE   [ 3.39]\n",
      ".               $KEEP     [ 9.54] --- $DELETE   [ 5.40] --- $APPEND_\" [ 4.61] --- $REPLACE_? [ 3.90] --- $UNKNOWN  [ 3.70]\n",
      "\n",
      "\u001b[37;1mTimestep:\u001b[0m 0  \n",
      "\u001b[37;1mRewards:\u001b[0m 0.000  \n",
      "\u001b[37;1mSource:\u001b[0m $START I believe he is tender and careful , but impatient and clumsy at the same time .  \n",
      "\u001b[37;1mOutput:\u001b[0m $START I believe he is tender and careful , but impatient and clumsy at the same time .  \n",
      "\n",
      "\u001b[37;1mTimestep:\u001b[0m \u001b[32;1m1\u001b[0m  \n",
      "\u001b[37;1mRewards:\u001b[0m -0.100  \n",
      "\u001b[37;1mSource:\u001b[0m $START I believe he is tender and careful , but impatient and clumsy at the same time .  \n",
      "\u001b[37;1mOutput:\u001b[0m $START I believe he is tender and careful , but impatient and clumsy at the same time .  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:133: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method should be an int or np.int64, actual type: <class 'list'>\u001b[0m\n",
      "  logger.warn(f\"{pre} should be an int or np.int64, actual type: {type(obs)}\")\n",
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:133: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method should be an int or np.int64, actual type: <class 'list'>\u001b[0m\n",
      "  logger.warn(f\"{pre} should be an int or np.int64, actual type: {type(obs)}\")\n",
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/core.py:57: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n",
      "/home/rajk/miniconda3/envs/drl-gec/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:297: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "state = env.reset()\n",
    "print(\"# References\")\n",
    "for ref in env.reference_tokens_list:\n",
    "    print(ref)\n",
    "print()\n",
    "done = False\n",
    "while not done:\n",
    "    with torch.no_grad():\n",
    "        [logits] = model([state])\n",
    "        actions = logits.argmax(-1)\n",
    "        actions = actions.cpu().numpy()\n",
    "        v, i = logits.topk(5)\n",
    "        v = v.cpu().numpy()\n",
    "        i = i.cpu().numpy()\n",
    "        for s, lp in zip(state, zip(env.labels[i], v)):\n",
    "            print(f\"{s:15}\", \" --- \".join(f\"{l:9} [{p:5.2f}]\" for (l, p) in zip(*lp)))\n",
    "        print()\n",
    "    next_state, reward, done, info = env.step(actions)\n",
    "    state = next_state\n",
    "    outputs = env.render()\n",
    "    for o in outputs:\n",
    "        print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a8cb1-2b94-49e3-aff9-504ea19cef04",
   "metadata": {},
   "source": [
    "# Close Google Compute Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "983c1785-751e-468b-a77d-3aab03328941",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_gce_instance():\n",
    "    !gcloud compute instances stop drl-gec --zone us-west1-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ab31a-6f54-442b-9c09-6fd1ab01759e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl-gec",
   "language": "python",
   "name": "drl-gec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
